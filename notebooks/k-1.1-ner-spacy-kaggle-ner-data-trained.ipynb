{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8187f2c9",
   "metadata": {},
   "source": [
    "Following the blog: https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0685380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "path ='/Users/kinga/Documents/4_th_Brain/glg_sandbox/'\n",
    "input_file = path  + \"Data/ner_dataset.json\"\n",
    "output_file = path + \"Data/dump1.p\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b1e67",
   "metadata": {},
   "source": [
    "Downloaded the 'ner_dataset.csv' dataset from kaggle: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csv part of which will be used to train the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7481cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docLoc = \"/Users/kinga/Documents/4_th_Brain/glg_sandbox/Data/ner_dataset.csv\"\n",
    "df = pd.read_csv(docLoc, encoding = \"ISO-8859-1\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e72042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25d7064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cbfe1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['Sentence #', \"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff9a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/ner_dataset.tsv', sep='\\t', encoding='utf-8', index=False) #converting the .csv to .tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3867934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert .tsv file to dataturks json format. \n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1  \n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d) \n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                          \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cda1b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_to_json_format(\"Data/ner_dataset.tsv\",'Data/ner_dataset.json','abc') #converting .tsv to .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4be08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='/Users/kinga/Documents/4_th_Brain/glg_sandbox/'\n",
    "input_file = path  + \"Data/ner_dataset.json\"\n",
    "output_file = path + \"Data/dump1.p\"\n",
    "try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(input_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "#         print(training_data)\n",
    "\n",
    "        with open(output_file, 'wb') as fp:\n",
    "            pickle.dump(training_data, fp)\n",
    "\n",
    "except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + input_file + \"\\n\" + \"error = \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5435837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87a49d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Word Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country ',\n",
       " {'entities': [(0, 4, 'Tag'),\n",
       "   (53, 59, 'B-geo'),\n",
       "   (82, 86, 'B-geo'),\n",
       "   (116, 123, 'B-gpe')]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2ebf3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47761"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d864fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training data \n",
    "with open (output_file, 'rb') as fp:\n",
    "    training_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e8a62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_count = 260 \n",
    "TRAIN_DATA = training_data[:260] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a585d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training additional entity types using spaCy\n",
    "from __future__ import unicode_literals, print_function\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0bdecaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngeo = Geographical Entity\\norg = Organization\\nper = Person\\ngpe = Geopolitical Entity\\ntim = Time indicator\\nart = Artifact\\neve = Event\\nnat = Natural Phenomenon\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New entity labels\n",
    "# Specify the new entity labels which you want to add here\n",
    "LABEL = ['I-geo', 'B-geo', 'I-art', 'B-art', 'B-tim', 'B-nat', 'B-eve', 'O', 'I-per', 'I-tim', 'I-nat', 'I-eve', 'B-per', 'I-org', 'B-gpe', 'B-org', 'I-gpe']\n",
    "\n",
    "\"\"\"\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59732d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')  # create blank Language class\n",
    "print(\"Created blank 'en' model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3fe2552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "77103e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the labels\n",
    "for i in LABEL:\n",
    "    ner.add_label(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "970528dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8def7b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E022] Could not find a transition with the name 'U-Tag' in the NER model.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-af7c3d3f6ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n\u001b[0;32m---> 11\u001b[0;31m                            losses=losses)\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitn\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Losses for iteration {itn} is {losses}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/glg1/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser._init_gold_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mner.pyx\u001b[0m in \u001b[0;36mspacy.syntax.ner.BiluoPushDown.preprocess_gold\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mner.pyx\u001b[0m in \u001b[0;36mspacy.syntax.ner.BiluoPushDown.lookup_transition\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E022] Could not find a transition with the name 'U-Tag' in the NER model.\""
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "        if itn%10 == 0:\n",
    "            print(f'Losses for iteration {itn} is {losses}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f0dfefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'Gianni Infantino is the president of FIFA.'\n",
      "B-per Gianni\n",
      "B-geo Infantino\n",
      "B-org FIFA\n",
      "B-tim .\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfc6e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to Data/\n"
     ]
    }
   ],
   "source": [
    "# output_dir = \"/Users/kinga/Documents/4_th_Brain/glg_sandbox/Data/\"\n",
    "\n",
    "# nlp.to_disk(\"model1\")\n",
    "# print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f93ce6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-per Gianni\n",
      "B-geo Infantino\n",
      "B-org FIFA\n",
      "B-tim .\n"
     ]
    }
   ],
   "source": [
    "# Test the saved model\n",
    "test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "# print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load('/Users/kinga/Documents/4_th_Brain/glg_sandbox/model1')\n",
    "doc2 = nlp2(test_text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df3eb7",
   "metadata": {},
   "source": [
    "Trying model on GLG Case Study text:\n",
    "\"EarthEnable installs affordable earthen floors in homes across Rwanda and Uganda, which helps mitigate health issues caused by dirt floors such as asthma, diarrhea, and malnutrition. The underside of EarthEnable’s flooring product had been suffering cracks and erosion at an unusually high rate, and they needed help diagnosing the cause.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c9c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'EarthEnable installs affordable earthen floors in homes across Rwanda and Uganda, which helps mitigate health issues caused by dirt floors such as asthma, diarrhea, and malnutrition. The underside of EarthEnable’s flooring product had been suffering cracks and erosion at an unusually high rate, and they needed help diagnosing the cause.'\n",
      "B-geo Rwanda\n",
      "B-geo Uganda\n",
      "B-geo .\n"
     ]
    }
   ],
   "source": [
    "test_text2 = \"EarthEnable installs affordable earthen floors in homes across Rwanda and Uganda, which helps mitigate health issues caused by dirt floors such as asthma, diarrhea, and malnutrition. The underside of EarthEnable’s flooring product had been suffering cracks and erosion at an unusually high rate, and they needed help diagnosing the cause.\"\n",
    "doc3 = nlp2(test_text2)\n",
    "print(\"Entities in '%s'\" % test_text2)\n",
    "for ent in doc3.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8392d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'A marketing team at a leading advertising technology company had difficulties understanding how today’s marketers in Southeast Asia measure advertising effectiveness across different segments. The team also sought to differentiate their advertising offerings from key competitors.'\n",
      "B-tim today\n",
      "B-geo Southeast\n",
      "I-geo Asia\n",
      "B-geo .\n",
      "B-geo .\n"
     ]
    }
   ],
   "source": [
    "test_text3 = \"A marketing team at a leading advertising technology company had difficulties understanding how today’s marketers in Southeast Asia measure advertising effectiveness across different segments. The team also sought to differentiate their advertising offerings from key competitors.\"\n",
    "doc4 = nlp2(test_text3)\n",
    "print(\"Entities in '%s'\" % test_text3)\n",
    "for ent in doc4.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9afea4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'In November 2019, Tesla announced the release of the Cybertruck, the company’s all electric pickup truck and the automaker’s sixth vehicle since its founding. While initial research had been conducted, GLG clients still had questions regarding consumer sentiment about electric trucks and where the luxury brand’s truck fit into the marketplace before making an investment decision.'\n",
      "B-tim November\n",
      "I-tim 2019\n",
      "B-per Tesla\n",
      "B-tim Cybertruck\n",
      "B-org .\n",
      "I-org While\n",
      "B-org GLG\n",
      "B-geo .\n"
     ]
    }
   ],
   "source": [
    "test_text4 = \"In November 2019, Tesla announced the release of the Cybertruck, the company’s all electric pickup truck and the automaker’s sixth vehicle since its founding. While initial research had been conducted, GLG clients still had questions regarding consumer sentiment about electric trucks and where the luxury brand’s truck fit into the marketplace before making an investment decision.\"\n",
    "doc5 = nlp2(test_text4)\n",
    "print(\"Entities in '%s'\" % test_text4)\n",
    "for ent in doc5.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5937c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
